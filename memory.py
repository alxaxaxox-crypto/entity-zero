# memory.py - Ù†Ø¸Ø§Ù… Ø§Ù„Ø°Ø§ÙƒØ±Ø© Ø§Ù„Ø·ÙˆÙŠÙ„Ø© Ù„Ù„ÙƒÙŠØ§Ù† ØµÙØ±
import json
import os
import time
from datetime import datetime

class SimpleMemory:
    """Ø°Ø§ÙƒØ±Ø© Ø¨Ø³ÙŠØ·Ø© Ù…Ø¨Ù†ÙŠØ© Ø¹Ù„Ù‰ JSON (Ø¨Ø¯ÙˆÙ† Ù…ÙƒØªØ¨Ø§Øª Ø®Ø§Ø±Ø¬ÙŠØ©)"""
    
    def __init__(self, db_path="./memory_db"):
        self.db_path = db_path
        self.memory_file = os.path.join(db_path, "conversations.json")
        self.users_file = os.path.join(db_path, "users.json")
        self.training_file = os.path.join(db_path, "training_data.jsonl")  # Ø¬Ø¯ÙŠØ¯
        self.grok_replies_file = os.path.join(db_path, "grok_masterpieces.json")  # Ø¬Ø¯ÙŠØ¯
        
        # Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù…Ø¬Ù„Ø¯ Ø¥Ø°Ø§ Ù…Ø§ Ù…ÙˆØ¬ÙˆØ¯
        os.makedirs(db_path, exist_ok=True)
        
        # ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
        self.conversations = self._load_json(self.memory_file, [])
        self.users = self._load_json(self.users_file, {})
        self.grok_replies = self._load_json(self.grok_replies_file, [])  # Ø¬Ø¯ÙŠØ¯
        
        # Ø¥Ù†Ø´Ø§Ø¡ Ù…Ù„Ù Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ø¥Ø°Ø§ Ù…Ùˆ Ù…ÙˆØ¬ÙˆØ¯
        if not os.path.exists(self.training_file):
            open(self.training_file, 'w', encoding='utf-8').close()
    
    def _load_json(self, filepath, default):
        """ØªØ­Ù…ÙŠÙ„ Ù…Ù„Ù JSON Ø£Ùˆ Ø±Ø¬Ø¹ Ù‚ÙŠÙ…Ø© Ø§ÙØªØ±Ø§Ø¶ÙŠØ©"""
        try:
            if os.path.exists(filepath):
                with open(filepath, 'r', encoding='utf-8') as f:
                    return json.load(f)
        except:
            pass
        return default
    
    def _save_json(self, filepath, data):
        """Ø­ÙØ¸ Ø¨ÙŠØ§Ù†Ø§Øª Ù„Ù€ JSON"""
        try:
            with open(filepath, 'w', encoding='utf-8') as f:
                json.dump(data, f, ensure_ascii=False, indent=2)
        except Exception as e:
            print(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ø­ÙØ¸ Ø§Ù„Ø°Ø§ÙƒØ±Ø©: {e}")
    
    def save_interaction(self, tweet_id, username, user_text, bot_reply, lang="ar", source="unknown"):
        """Ø­ÙØ¸ ØªÙØ§Ø¹Ù„ Ø¬Ø¯ÙŠØ¯ Ù…Ø¹ Ù…ØµØ¯Ø± Ø§Ù„Ø±Ø¯ (Grok Ø£Ùˆ Ollama)"""
        timestamp = time.time()
        
        # 1. Ø­ÙØ¸ Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø©
        conversation = {
            "id": f"{tweet_id}_{int(timestamp)}",
            "tweet_id": tweet_id,
            "username": username or "unknown",
            "user_text": user_text,
            "bot_reply": bot_reply,
            "lang": lang,
            "source": source,  # Ø¬Ø¯ÙŠØ¯: Grok Ø£Ùˆ Ollama
            "timestamp": timestamp,
            "date": datetime.now().strftime("%Y-%m-%d %H:%M")
        }
        self.conversations.append(conversation)
        
        # 2. Ø¥Ø°Ø§ Ø§Ù„Ø±Ø¯ Ù…Ù† GrokØŒ Ø­ÙØ¸Ù‡ ÙƒÙ€ " masterpiece"
        if source == "grok":
            self.grok_replies.append({
                "user_text": user_text,
                "bot_reply": bot_reply,
                "lang": lang,
                "date": datetime.now().strftime("%Y-%m-%d %H:%M")
            })
            self._save_json(self.grok_replies_file, self.grok_replies)
            print(f"ðŸŽ¯ ØªÙ… Ø­ÙØ¸ Ø±Ø¯ Grok ÙƒÙ€ Masterpiece!")
        
        # 3. ØªØ­Ø¯ÙŠØ« Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…
        if username:
            if username not in self.users:
                self.users[username] = {
                    "first_seen": timestamp,
                    "interactions_count": 0,
                    "preferred_lang": lang,
                    "mood_history": [],
                    "topics": []
                }
            
            user = self.users[username]
            user["last_seen"] = timestamp
            user["interactions_count"] += 1
            user["last_bot_reply"] = bot_reply
            
            # ØªØ­Ù„ÙŠÙ„ Ø¨Ø³ÙŠØ· Ù„Ù„Ù…ÙˆØ¶ÙˆØ¹
            topic = " ".join(user_text.split()[:3])
            user["topics"].append(topic)
            if len(user["topics"]) > 10:
                user["topics"].pop(0)
        
        # 4. Ø­ÙØ¸ Ù„Ù„Ù…Ù„ÙØ§Øª
        self._save_json(self.memory_file, self.conversations)
        self._save_json(self.users_file, self.users)
        
        print(f"ðŸ’¾ ØªÙ… Ø­ÙØ¸ Ø§Ù„ØªÙØ§Ø¹Ù„: @{username} -> {tweet_id} (Ø§Ù„Ù…ØµØ¯Ø±: {source})")
    
    def save_for_training(self, user_text, bot_reply, lang="ar"):
        """Ø­ÙØ¸ Ø²ÙˆØ¬ (Ø³Ø¤Ø§Ù„/Ø¬ÙˆØ§Ø¨) Ù„ØªØ¯Ø±ÙŠØ¨ Ollama"""
        training_item = {
            "prompt": user_text,
            "response": bot_reply,
            "lang": lang,
            "date": datetime.now().strftime("%Y-%m-%d %H:%M"),
            "quality_score": len(bot_reply)  # Ø¨Ø³ÙŠØ·: Ø§Ù„Ø±Ø¯ÙˆØ¯ Ø§Ù„Ø£Ø·ÙˆÙ„ Ø£Ø­Ø³Ù†
        }
        
        # Ø¥Ù„Ø­Ø§Ù‚ Ù„Ù„Ù…Ù„Ù
        with open(self.training_file, 'a', encoding='utf-8') as f:
            f.write(json.dumps(training_item, ensure_ascii=False) + "\n")
        
        print(f"ðŸŽ“ ØªÙ… Ø¥Ø¶Ø§ÙØ© Ø¹ÙŠÙ†Ø© ØªØ¯Ø±ÙŠØ¨: {user_text[:30]}...")
    
    def get_training_data(self, min_quality=50, max_items=100):
        """Ø¬Ù„Ø¨ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ø¹Ø§Ù„ÙŠØ© Ø§Ù„Ø¬ÙˆØ¯Ø©"""
        training_data = []
        try:
            with open(self.training_file, 'r', encoding='utf-8') as f:
                for line in f:
                    try:
                        item = json.loads(line.strip())
                        if item.get('quality_score', 0) >= min_quality:
                            training_data.append(item)
                    except:
                        continue
        except:
            pass
        
        # Ø±Ø¬Ø¹ Ø§Ù„Ø£Ø­Ø¯Ø« ÙˆØ§Ù„Ø£ÙØ¶Ù„
        return sorted(training_data, key=lambda x: x.get('quality_score', 0), reverse=True)[:max_items]
    
    def get_grok_masterpieces(self, topic=None, max_items=5):
        """Ø¬Ù„Ø¨ Ø±Ø¯ÙˆØ¯ Grok Ø§Ù„Ø£ÙØ¶Ù„ (Ù„ØªØ¹Ù„Ù… Ollama)"""
        if not self.grok_replies:
            return []
        
        if topic:
            # ÙÙ„ØªØ±Ø© Ø­Ø³Ø¨ Ø§Ù„Ù…ÙˆØ¶ÙˆØ¹
            filtered = [
                r for r in self.grok_replies 
                if topic.lower() in r['user_text'].lower()
            ]
            return filtered[-max_items:]
        
        # Ø±Ø¬Ø¹ Ø§Ù„Ø£Ø­Ø¯Ø«
        return self.grok_replies[-max_items:]
    
    def get_user_context(self, username, max_items=3):
        """Ø¬Ù„Ø¨ Ø³ÙŠØ§Ù‚ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… Ø§Ù„Ø³Ø§Ø¨Ù‚"""
        if not username or username not in self.users:
            return None
        
        user = self.users[username]
        
        user_conversations = [
            c for c in reversed(self.conversations) 
            if c["username"] == username
        ][:max_items]
        
        if not user_conversations:
            return None
        
        context = f"Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… @{username} ØªÙØ§Ø¹Ù„ Ù…Ø¹Ùƒ {user['interactions_count']} Ù…Ø±Ø© Ø³Ø§Ø¨Ù‚Ø§Ù‹.\n"
        context += f"Ø¢Ø®Ø± Ù…ÙˆØ§Ø¶ÙŠØ¹: {', '.join(user['topics'][-3:])}\n"
        context += "Ø¢Ø®Ø± Ø±Ø¯ÙˆØ¯Ùƒ Ù„Ù‡:\n"
        
        for i, conv in enumerate(user_conversations[:2], 1):
            context += f"{i}. Ù‡Ùˆ Ù‚Ø§Ù„: '{conv['user_text'][:50]}...' -> Ø£Ù†Øª Ø±Ø¯ÙŠØª: '{conv['bot_reply'][:50]}...'\n"
        
        return context
    
    def get_similar_interactions(self, text, max_results=2):
        """Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† ØªÙØ§Ø¹Ù„Ø§Øª Ù…Ø´Ø§Ø¨Ù‡Ø©"""
        words = set(text.lower().split())
        matches = []
        
        for conv in reversed(self.conversations[-100:]):
            conv_words = set(conv["user_text"].lower().split())
            common = words & conv_words
            
            if len(common) >= 2:
                matches.append((len(common), conv))
        
        matches.sort(reverse=True)
        return [m[1] for m in matches[:max_results]]
    
    def get_stats(self):
        """Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„Ø°Ø§ÙƒØ±Ø©"""
        training_count = 0
        try:
            with open(self.training_file, 'r', encoding='utf-8') as f:
                training_count = sum(1 for _ in f if _.strip())
        except:
            pass
        
        return {
            "total_conversations": len(self.conversations),
            "unique_users": len(self.users),
            "grok_masterpieces": len(self.grok_replies),
            "training_samples": training_count,
            "last_interaction": self.conversations[-1]["date"] if self.conversations else "Ù„Ø§ ÙŠÙˆØ¬Ø¯"
        }
    
    def export_for_ollama_training(self, output_file="./memory_db/ollama_train.txt"):
        """ØªØµØ¯ÙŠØ± Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ø¨ØµÙŠØºØ© Ollama Modelfile"""
        training_data = self.get_training_data(min_quality=30, max_items=200)
        
        with open(output_file, 'w', encoding='utf-8') as f:
            f.write("# Training data for Ollama fine-tuning\n\n")
            for item in training_data:
                f.write(f"### User: {item['prompt']}\n")
                f.write(f"### Assistant: {item['response']}\n\n")
        
        print(f"ðŸ“¤ ØªÙ… ØªØµØ¯ÙŠØ± {len(training_data)} Ø¹ÙŠÙ†Ø© Ù„Ù€ Ollama: {output_file}")
        return len(training_data)